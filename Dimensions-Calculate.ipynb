{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import nltk\r\n",
    "import math\r\n",
    "import pickle\r\n",
    "import pyphen\r\n",
    "import itertools\r\n",
    "import numpy  as np\r\n",
    "import pandas as pd\r\n",
    "from tqdm.notebook  import tqdm\r\n",
    "from gensim.models  import TfidfModel\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "from nltk.corpus    import sentiwordnet as swn\r\n",
    "from functions      import readSet, columnNames, divide, add, dirs\r\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordlists\n",
    "sylTool = pyphen.Pyphen(lang='en_US') #syllables\n",
    "difficultWordsSAT  = readSet('./wordlists/difficultWordsSAT.txt')\n",
    "easyWordsDaleChall = readSet('./wordlists/easyWordsDaleChall.txt')\n",
    "postags            = readSet('./wordlists/postags.txt')\n",
    "slangWords         = readSet('./wordlists/slang.txt')\n",
    "positiveWords      = readSet('./wordlists/positiveWords.txt')\n",
    "negativeWords      = readSet('./wordlists/negativeWords.txt')\n",
    "strongWords = positiveWords.union(negativeWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrLetters')\n",
    "def nrLetters(row):\n",
    "    vector = [len(word) for word in row['words']]\n",
    "    return np.sum(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrWords')\n",
    "def nrWords(row):\n",
    "    return len(row['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrSentences')\n",
    "def nrSentences(row):\n",
    "    return len(row['sentences'].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames(*postags)\n",
    "def nrPOSTags(row):\n",
    "    lst = nltk.pos_tag(row['words'])\n",
    "    tags = [token[1] for token in lst]\n",
    "    dct = dict(zip(postags, np.zeros(len(postags)))) #zero for each tag\n",
    "    for tag in tags:\n",
    "        if tag in postags:\n",
    "            dct[tag] += 1\n",
    "    ret = [dct[tag] for tag in postags]\n",
    "    return tuple(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrSyllables', 'nrMonoSyllables', 'nrBiSyllables', 'nrPolySyllables')\n",
    "def nrSyllables(row):\n",
    "    s = 0\n",
    "    mono = 0\n",
    "    bi = 0\n",
    "    poly = 0\n",
    "    for word in row['words']:\n",
    "        syllables = len(sylTool.inserted(word).split('-'))\n",
    "        s += syllables\n",
    "\n",
    "        if syllables == 1:\n",
    "            mono += 1\n",
    "        if syllables == 2:\n",
    "            bi += 1\n",
    "        if syllables >= 3:\n",
    "            poly += 1\n",
    "\n",
    "    return s, mono, bi, poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrDifficultWordsSAT')\n",
    "def nrDifficultWordsSAT(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if word in difficultWordsSAT:\n",
    "            s += 1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@columnNames('nrDifficultWordsDaleChall')\n",
    "def nrDifficultWordsDaleChall(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if word not in easyWordsDaleChall:\n",
    "            s += 1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrLongWords')\n",
    "def nrLongWords(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if len(word) >= 6:\n",
    "            s += 1\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrSynsets')\n",
    "def nrSynsets(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        s += len([x for x in swn.senti_synsets(word)])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrSlangWords')\n",
    "def nrSlangWords(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if word in slangWords:\n",
    "            s += 1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('uniquenessMean', 'uniquenessSTD')\n",
    "def uniqueness(row, dct, tfidf):\n",
    "    bow = dct.doc2bow(row['words'])\n",
    "    vector = [tupl[1] for tupl in tfidf[bow]]\n",
    "    return np.mean(vector), np.std(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Enablers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceOpinion(text):\n",
    "    words = text.split()\n",
    "    synsets = []\n",
    "    for word in words:\n",
    "        scores = [(x.pos_score(), x.neg_score(), x.obj_score()) for x in swn.senti_synsets(word)]\n",
    "        if len(scores) > 0:\n",
    "            synsets.append(np.mean(scores, axis=0))\n",
    "    score = np.mean(synsets, axis=0)\n",
    "    if np.isscalar(score): #weird hack to check for nan\n",
    "        return 0\n",
    "    if score[0] > score[1]:\n",
    "        return 1 #positive\n",
    "    return -1 #negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('opinionPolarity')\r\n",
    "def opinionPolarity(row):\r\n",
    "    sentences = row['sentences'].split(',')\r\n",
    "    pos = 1\r\n",
    "    neg = 1\r\n",
    "    for sent in sentences:\r\n",
    "        op = sentenceOpinion(sent)\r\n",
    "        if op > 0:\r\n",
    "            pos += 1\r\n",
    "        else:\r\n",
    "            neg += 1\r\n",
    "    minimun = min([pos, neg])\r\n",
    "    maximun = max([pos, neg])\r\n",
    "    return maximun / minimun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguousSentimentWord(word):\n",
    "    synsets = [[x.pos_score(), x.neg_score(), x.obj_score()] for x in swn.senti_synsets(word)]\n",
    "    pos = False\n",
    "    neg = False\n",
    "    for s in synsets:\n",
    "        if s[2] != max(s): #not objective\n",
    "            if s[0] > s[1]:\n",
    "                pos = True\n",
    "            else:\n",
    "                neg = True\n",
    "    \n",
    "    if pos and neg:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrAmbiguousSentimentWords')\n",
    "def nrAmbiguousSentimentWords(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        s += ambiguousSentimentWord(word)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrStrongSentimentWords')\n",
    "def nrStrongSentimentWords(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if word in strongWords:\n",
    "            s += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaLIX')\n",
    "def formulaLIX(row):\n",
    "    first  = row['nrWords/nrSentences']\n",
    "    second = row['nrLongWords/nrWords']\n",
    "    return first + (second * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaFleshKincaid')\n",
    "def formulaFleshKincaid(row):\n",
    "    first  = row['nrWords/nrSentences']\n",
    "    second = row['nrSyllables/nrWords']\n",
    "    return 206.835 - (1.015 * first) - (84.6 * second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaSMOG')\n",
    "def formulaSMOG(row):\n",
    "    first = row['nrPolySyllables/nrSentences']\n",
    "    return 1.043 * math.sqrt((first * 30) + 3.1291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaGunningFog')\n",
    "def formulaGunningFog(row):\n",
    "    first  = row['nrWords/nrSentences']\n",
    "    second = row['nrPolySyllables/nrWords']\n",
    "    return (first + second) * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaDaleChall')\n",
    "def formulaDaleChall(row):\n",
    "    first  = row['nrDifficultWordsDaleChall/nrWords']\n",
    "    second = row['nrWords/nrSentences']\n",
    "    return (0.1579 * first * 100) + (0.0496 * second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaColemanLiau')\n",
    "def formulaColemanLiau(row):\n",
    "    L = row['nrLetters/nrWords']\n",
    "    S = row['nrSentences'] / row['nrWords']\n",
    "    return (0.0588 * L * 100) - (0.296 * S * 100) - 15.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaLinsearWrite')\n",
    "def formulaLinsearWrite(row):\n",
    "    easyWords = row['nrMonoSyllables'] + row['nrBiSyllables']\n",
    "    hardWords = row['nrPolySyllables'] * 3\n",
    "    score = (easyWords + hardWords) / row['nrSentences']\n",
    "    if score > 20:\n",
    "        score = score / 2\n",
    "    else:\n",
    "        score = (score / 2) - 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " @columnNames('formulaSpacheSAT', 'formulaSpacheDaleChall')\n",
    " def formulaSpache(row):\n",
    "     first           = row['nrWords/nrSentences']\n",
    "     secondSAT       = row['nrDifficultWordsSAT/nrWords']\n",
    "     secondDaleChall = row['nrDifficultWordsDaleChall/nrWords']\n",
    "     scoreSAT       = (0.121 * first) + (0.082 * secondSAT)       + 0.659\n",
    "     scoreDaleChall = (0.121 * first) + (0.082 * secondDaleChall) + 0.659\n",
    "     return scoreSAT, scoreDaleChall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaFORCAST')\n",
    "def formulaFORCAST(row):\n",
    "    N = row['nrMonoSyllables/nrWords'] * 150\n",
    "    return 20 - (N/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets = dirs('./data')\r\n",
    "#datasets = ['IMDB']\r\n",
    "#datasets = ['Sentiment140']\r\n",
    "datasets = ['AirlineTweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e4bd5fab484b1a8d30eda87481210b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e72ccd1a2254dd7a5962388d8d302a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset in tqdm(datasets, desc=\"Datasets\"):\r\n",
    "    dataFile  = f'./data/{dataset}/Data-Cleaned.csv'\r\n",
    "    outputFile = f'./data/{dataset}/Dimensions.csv'\r\n",
    "    tfidfFile  = f'./models/{dataset}/TF-IDF.model'\r\n",
    "    dictFile   = f'./models/{dataset}/Dictionary.model'\r\n",
    "\r\n",
    "\r\n",
    "    if not os.path.exists(dataFile):\r\n",
    "        raise ValueError(f\"Dataset {dataset} has not been cleaned\")\r\n",
    "\r\n",
    "    if not os.path.exists(tfidfFile):\r\n",
    "        raise ValueError(f\"TFIDF for {dataset} has not been created\")\r\n",
    "\r\n",
    "    tfidf = TfidfModel.load(tfidfFile)\r\n",
    "    dct   = Dictionary.load(dictFile)\r\n",
    "\r\n",
    "    data = pd.read_csv(dataFile)\r\n",
    "\r\n",
    "    if os.path.exists(outputFile):\r\n",
    "        df = pd.read_csv(outputFile)\r\n",
    "    else:\r\n",
    "        df = pd.DataFrame()\r\n",
    "\r\n",
    "    df['sentences'] = data['sentences']\r\n",
    "    df['words'] = data.progress_apply(lambda row: row['text'].split(), axis=1)\r\n",
    "\r\n",
    "    ###Base variables\r\n",
    "    base = [\r\n",
    "        nrLetters(df),\r\n",
    "        nrSyllables(df), #including mono, bi and poly counts\r\n",
    "        nrWords(df),\r\n",
    "        nrSentences(df),\r\n",
    "        nrDifficultWordsSAT(df),\r\n",
    "        nrDifficultWordsDaleChall(df),\r\n",
    "        nrLongWords(df),\r\n",
    "        nrSynsets(df),\r\n",
    "        nrSlangWords(df),\r\n",
    "    ]\r\n",
    "    postagsBase = [\r\n",
    "        nrPOSTags(df),\r\n",
    "    ]\r\n",
    "    postag = [\r\n",
    "        add(df, ['CC','IN'],          into='nrConjunctions'),\r\n",
    "        add(df, ['JJ','JJR','JJS'],   into='nrAdjectives'),\r\n",
    "        add(df, ['RB','RBR','RBS'],   into='nrAdverbs'),\r\n",
    "        add(df, ['MD','VBG'],         into='nrComplexVerbs'),\r\n",
    "        add(df, ['POS','PRP','PRP$'], into='nrPossesives'),\r\n",
    "        add(df, ['DT','PDT'],         into='nrDeterminers'),\r\n",
    "    ]\r\n",
    "    postagwords = [\r\n",
    "        divide(df, list(itertools.chain(*postag)), by='nrWords')\r\n",
    "    ]\r\n",
    "\r\n",
    "    ###Lexical metrics\r\n",
    "    lexical = [\r\n",
    "        uniqueness(df, dct, tfidf),\r\n",
    "        divide(df, [\r\n",
    "            'nrLetters',\r\n",
    "            'nrSyllables',\r\n",
    "            'nrMonoSyllables',\r\n",
    "            'nrBiSyllables',\r\n",
    "            'nrPolySyllables',\r\n",
    "            'nrLongWords',],\r\n",
    "            by='nrWords'\r\n",
    "        ),\r\n",
    "    ]\r\n",
    "\r\n",
    "    ###Syntactic metrics\r\n",
    "    syntactic = [\r\n",
    "        divide(df, [\r\n",
    "            'nrLetters',\r\n",
    "            'nrWords',\r\n",
    "            'nrSyllables',\r\n",
    "            'nrMonoSyllables',\r\n",
    "            'nrBiSyllables',\r\n",
    "            'nrPolySyllables',\r\n",
    "            'nrLongWords',],\r\n",
    "            by='nrSentences'\r\n",
    "        )\r\n",
    "    ]\r\n",
    "\r\n",
    "    ###Semantic metrics\r\n",
    "    semantic = [\r\n",
    "        divide(df, [\r\n",
    "            'nrDifficultWordsSAT',\r\n",
    "            'nrDifficultWordsDaleChall',\r\n",
    "            'nrSynsets',\r\n",
    "            'nrSlangWords'],\r\n",
    "            by='nrWords'\r\n",
    "        )\r\n",
    "    ]\r\n",
    "\r\n",
    "    ###Sentiment Enablers\r\n",
    "    sentiment = [\r\n",
    "        opinionPolarity(df),\r\n",
    "        nrAmbiguousSentimentWords(df),\r\n",
    "        nrStrongSentimentWords(df),\r\n",
    "        divide(df, [\r\n",
    "            'nrAmbiguousSentimentWords',\r\n",
    "            'nrStrongSentimentWords'],\r\n",
    "            by='nrWords'\r\n",
    "        )\r\n",
    "    ]\r\n",
    "\r\n",
    "    ###Readability formulas\r\n",
    "    formulas = [\r\n",
    "        formulaFleshKincaid(df),\r\n",
    "        formulaGunningFog(df),\r\n",
    "        formulaSMOG(df),\r\n",
    "        formulaDaleChall(df),\r\n",
    "        formulaColemanLiau(df),\r\n",
    "        formulaLinsearWrite(df),\r\n",
    "        formulaSpache(df),\r\n",
    "        formulaLIX(df),\r\n",
    "        formulaFORCAST(df),\r\n",
    "    ]\r\n",
    "\r\n",
    "    df = df.drop(columns=['words', 'sentences'])\r\n",
    "    df.to_csv(outputFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = {\n",
    "    'base':        list(itertools.chain(*base)),\n",
    "    'postag':      list(itertools.chain(*postag)),\n",
    "    'postagwords': list(itertools.chain(*postagwords)),\n",
    "    'lexical':     list(itertools.chain(*lexical)),\n",
    "    'syntactic':   list(itertools.chain(*syntactic)),\n",
    "    'semantic':    list(itertools.chain(*semantic)),\n",
    "    'sentiment':   list(itertools.chain(*sentiment)),\n",
    "    'formulas':    list(itertools.chain(*formulas)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dimensions, open('./data/Dimensions-All.pickle', mode='wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python385jvsc74a57bd0c44b72c526c8c2d5c61f5343e5a1f2700c1b4b253758e0d185049d36041666fd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}