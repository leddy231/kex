{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0c44b72c526c8c2d5c61f5343e5a1f2700c1b4b253758e0d185049d36041666fd",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pickle\n",
    "import pyphen\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from functions import readSet, columnNames, divide, add\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordlists\n",
    "sylTool = pyphen.Pyphen(lang='en_US') #syllables\n",
    "difficultWordsSAT  = readSet('./wordlists/difficultWordsSAT.txt')\n",
    "easyWordsDaleChall = readSet('./wordlists/easyWordsDaleChall.txt')\n",
    "postags            = readSet('./wordlists/postags.txt')\n",
    "slangWords         = readSet('./wordlists/slang.txt')"
   ]
  },
  {
   "source": [
    "# Base variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrLetters')\n",
    "def nrLetters(row):\n",
    "    vector = [len(word) for word in row['words']]\n",
    "    return np.sum(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrWords')\n",
    "def nrWords(row):\n",
    "    return len(row['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrSentences')\n",
    "def nrSentences(row):\n",
    "    return len(row['sentences'].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames(*postags)\n",
    "def nrPOSTags(row):\n",
    "    lst = nltk.pos_tag(row['words'])\n",
    "    tags = [token[1] for token in lst]\n",
    "    dct = dict(zip(postags, np.zeros(len(postags)))) #zero for each tag\n",
    "    for tag in tags:\n",
    "        if tag in postags:\n",
    "            dct[tag] += 1\n",
    "    ret = [dct[tag] for tag in postags]\n",
    "    return tuple(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrSyllables', 'nrMonoSyllables', 'nrBiSyllables', 'nrPolySyllables')\n",
    "def nrSyllables(row):\n",
    "    s = 0\n",
    "    mono = 0\n",
    "    bi = 0\n",
    "    poly = 0\n",
    "    for word in row['words']:\n",
    "        syllables = len(sylTool.inserted(word).split('-'))\n",
    "        s += syllables\n",
    "\n",
    "        if syllables == 1:\n",
    "            mono += 1\n",
    "        if syllables == 2:\n",
    "            bi += 1\n",
    "        if syllables >= 3:\n",
    "            poly += 1\n",
    "\n",
    "    return s, mono, bi, poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrDifficultWordsSAT')\n",
    "def nrDifficultWordsSAT(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if word in difficultWordsSAT:\n",
    "            s += 1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@columnNames('nrDifficultWordsDaleChall')\n",
    "def nrDifficultWordsDaleChall(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if word not in easyWordsDaleChall:\n",
    "            s += 1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrLongWords')\n",
    "def nrLongWords(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if len(word) >= 6:\n",
    "            s += 1\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrSynsets')\n",
    "def nrSynsets(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        s += len([x for x in swn.senti_synsets(word)])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrSlangWords')\n",
    "def nrSlangWords(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if word in slangWords:\n",
    "            s += 1\n",
    "\n",
    "    return s"
   ]
  },
  {
   "source": [
    "# Lexical metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('meanUniqueness', 'stdUniqueness')\n",
    "def uniqueness(row, dct, tfidf):\n",
    "    bow = dct.doc2bow(row['words'])\n",
    "    vector = [tupl[1] for tupl in tfidf[bow]]\n",
    "    return np.mean(vector), np.std(vector)"
   ]
  },
  {
   "source": [
    "# Sentiment Enablers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceOpinion(text):\n",
    "    words = text.split()\n",
    "    synsets = []\n",
    "    for word in words:\n",
    "        scores = [(x.pos_score(), x.neg_score(), x.obj_score()) for x in swn.senti_synsets(word)]\n",
    "        if len(scores) > 0:\n",
    "            synsets.append(np.mean(scores, axis=0))\n",
    "    score = np.mean(synsets, axis=0)\n",
    "    if np.isscalar(score): #weird hack to check for nan\n",
    "        return 0\n",
    "    if score[0] > score[1]:\n",
    "        return 1 #positive\n",
    "    return -1 #negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('opinionMixScore')\n",
    "def opinionMixScore(row):\n",
    "    sentences = row['sentences'].split(',')\n",
    "    pos = 1\n",
    "    neg = 1\n",
    "    for sent in sentences:\n",
    "        op = sentenceOpinion(sent)\n",
    "        if op > 0:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "    minimun = min([pos, neg])\n",
    "    maximun = max([pos, neg])\n",
    "    return maximun / minimun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambiguousSentimentWord(word):\n",
    "    synsets = [[x.pos_score(), x.neg_score(), x.obj_score()] for x in swn.senti_synsets(word)]\n",
    "    pos = False\n",
    "    neg = False\n",
    "    for s in synsets:\n",
    "        if s[2] != max(s): #not objective\n",
    "            if s[0] > s[1]:\n",
    "                pos = True\n",
    "            else:\n",
    "                neg = True\n",
    "    \n",
    "    if pos and neg:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrAmbiguousSentimentWords')\n",
    "def nrAmbiguousSentimentWords(row):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        s += ambiguousSentimentWord(word)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('nrStrongSentimentWords')\n",
    "def nrStrongSentimentWords(row, strongWords):\n",
    "    s = 0\n",
    "    for word in row['words']:\n",
    "        if word in strongWords:\n",
    "            s += 1\n",
    "    return s"
   ]
  },
  {
   "source": [
    "# Readability formulas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaLIX')\n",
    "def formulaLIX(row):\n",
    "    first  = row['nrWords/nrSentences']\n",
    "    second = row['nrLongWords/nrWords']\n",
    "    return first + (second * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaFleshKincaid')\n",
    "def formulaFleshKincaid(row):\n",
    "    first  = row['nrWords/nrSentences']\n",
    "    second = row['nrSyllables/nrWords']\n",
    "    return 206.835 - (1.015 * first) - (84.6 * second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaSMOG')\n",
    "def formulaSMOG(row):\n",
    "    first = row['nrPolySyllables/nrSentences']\n",
    "    return 1.043 * math.sqrt((first * 30) + 3.1291)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaGunningFog')\n",
    "def formulaGunningFog(row):\n",
    "    first  = row['nrWords/nrSentences']\n",
    "    second = row['nrPolySyllables/nrWords']\n",
    "    return (first + second) * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaDaleChall')\n",
    "def formulaDaleChall(row):\n",
    "    first  = row['nrDifficultWordsDaleChall/nrWords']\n",
    "    second = row['nrWords/nrSentences']\n",
    "    return (0.1579 * first * 100) + (0.0496 * second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaColemanLiau')\n",
    "def formulaColemanLiau(row):\n",
    "    L = row['nrLetters/nrWords']\n",
    "    S = row['nrSentences'] / row['nrWords']\n",
    "    return (0.0588 * L * 100) - (0.296 * S * 100) - 15.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaLinsearWrite')\n",
    "def formulaLinsearWrite(row):\n",
    "    easyWords = row['nrMonoSyllables'] + row['nrBiSyllables']\n",
    "    hardWords = row['nrPolySyllables'] * 3\n",
    "    score = (easyWords + hardWords) / row['nrSentences']\n",
    "    if score > 20:\n",
    "        score = score / 2\n",
    "    else:\n",
    "        score = (score / 2) - 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " @columnNames('formulaSpacheSAT', 'formulaSpacheDaleChall')\n",
    " def formulaSpache(row):\n",
    "     first           = row['nrWords/nrSentences']\n",
    "     secondSAT       = row['nrDifficultWordsSAT/nrWords']\n",
    "     secondDaleChall = row['nrDifficultWordsDaleChall/nrWords']\n",
    "     scoreSAT       = (0.121 * first) + (0.082 * secondSAT)       + 0.659\n",
    "     scoreDaleChall = (0.121 * first) + (0.082 * secondDaleChall) + 0.659\n",
    "     return scoreSAT, scoreDaleChall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@columnNames('formulaFORCAST')\n",
    "def formulaFORCAST(row):\n",
    "    N = row['nrMonoSyllables/nrWords'] * 150\n",
    "    return 20 - (N/10)"
   ]
  },
  {
   "source": [
    "# Applying to datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets = os.listdir('./data')\n",
    "datasets = ['AirlineTweets']\n",
    "#datasets = ['Sentiment140']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Datasets:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5741e29cca0468c92c249f6ddd7933c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88776bc164be4fdd94a43578f05400e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86091a848654425d98cc7ad4ff4edfb1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02b58e8f233a429e990501a57a7e447d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf2531c9ffb44592a61cf19213c0aa8a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10eae3a299a14ba18c9f35a0e222a657"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1947a17e7714ef0bc0f253e95df1d19"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d23b05a21ec4118bed20ade80729179"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1735d3efc6924378b0b4b7013b12504d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb8060896da34741a4fdd1d2ae9c6be7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa217785b5974e5a8dbbb44813cb97ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fd54acc6c1842cda988b0d0df2f894d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f0de54720bd49cbb586fc77b5f26e9b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "799e1f0bddc749aa8361e7850ed9a814"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Libraries\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n  return _methods._mean(a, axis=axis, dtype=dtype,\nC:\\Libraries\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bee5beca86e4c4fb346d558d484f0f7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b33f19cd63b4243bfe26c4407a91459"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47c023f5c3024a68bf1c3208591dc0bd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "815f4d25744c450191a351a2ef2dc23b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49671c3305d84e1fb4ed9e7722d3aaa0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17ebe66072ed4046a1d3d8e8e7ac9cea"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "baadeb7a0811493dae2cee8970cafa5d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad520bc303d44deca2f1636077323f73"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a2328d7dc924444982a3a6b45c239fe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02071e272f334ee29d78e6714dcdb95b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/11541 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9189f998b014f669fd9c0141aa32b0c"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "for dataset in tqdm(datasets, desc=\"Datasets\"):\n",
    "    inputFile  = f'./data/{dataset}/Data-Predicted.csv'\n",
    "    outputFile = f'./data/{dataset}/Data-With-Dimensions.csv'\n",
    "    tfidfFile  = f'./models/{dataset}/TF-IDF.model'\n",
    "    dictFile   = f'./models/{dataset}/Dictionary.model'\n",
    "\n",
    "    positiveWords = [\"good\", \"nice\", \"cool\", \"lovely\", \"wonderful\", \"great\", \"awesome\", \"fantastic\", \"amazing\", \"fun\", \"excellent\"]\n",
    "    negativeWords = [\"bad\", \"horrible\", \"terrible\", \"awful\", \"worst\", \"shitty\", \"crappy\", \"sucks\", \"hate\"]\n",
    "    strongWords = set(positiveWords + negativeWords)\n",
    "    \n",
    "    if not os.path.exists(inputFile):\n",
    "        raise ValueError(f\"Dataset {dataset} has not been predicted\")\n",
    "\n",
    "    if not os.path.exists(tfidfFile):\n",
    "        raise ValueError(f\"TFIDF for {dataset} has not been created\")\n",
    "\n",
    "    tfidf = TfidfModel.load(tfidfFile)\n",
    "    dct   = Dictionary.load(dictFile)\n",
    "\n",
    "    #df = pd.read_csv(inputFile)\n",
    "    if os.path.exists(outputFile):\n",
    "        df = pd.read_csv(outputFile)\n",
    "    else:\n",
    "        df = pd.read_csv(inputFile)\n",
    "\n",
    "    df['words'] = df.progress_apply(lambda row: row['text'].split(), axis=1)\n",
    "\n",
    "    ###Base variables\n",
    "    base = [\n",
    "        nrLetters(df),\n",
    "        nrSyllables(df), #including mono, bi and poly counts\n",
    "        nrWords(df),\n",
    "        nrSentences(df),\n",
    "        nrPOSTags(df),\n",
    "        nrDifficultWordsSAT(df),\n",
    "        nrDifficultWordsDaleChall(df),\n",
    "        nrLongWords(df),\n",
    "        nrSynsets(df),\n",
    "        nrSlangWords(df),\n",
    "    ]\n",
    "    postag = [\n",
    "        add(df, ['CC','IN'],          into='nrConjunctions'),\n",
    "        add(df, ['JJ','JJR','JJS'],   into='nrAdjectives'),\n",
    "        add(df, ['RB','RBR','RBS'],   into='nrAdverbs'),\n",
    "        add(df, ['MD','VBG'],         into='nrComplexVerbs'),\n",
    "        add(df, ['POS','PRP','PRP$'], into='nrPossesives'),\n",
    "        add(df, ['DT','PDT'],         into='nrDeterminers'),\n",
    "    ]\n",
    "    postagwords = [\n",
    "        divide(df, list(itertools.chain(*postag)), by='nrWords')\n",
    "    ]\n",
    "\n",
    "    ###Lexical metrics\n",
    "    lexical = [\n",
    "        uniqueness(df, dct, tfidf),\n",
    "        divide(df, [\n",
    "            'nrLetters',\n",
    "            'nrSyllables',\n",
    "            'nrMonoSyllables',\n",
    "            'nrBiSyllables',\n",
    "            'nrPolySyllables',\n",
    "            'nrLongWords',],\n",
    "            by='nrWords'\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    ###Syntactic metrics\n",
    "    syntactic = [\n",
    "        divide(df, [\n",
    "            'nrLetters',\n",
    "            'nrWords',\n",
    "            'nrSyllables',\n",
    "            'nrMonoSyllables',\n",
    "            'nrBiSyllables',\n",
    "            'nrPolySyllables',\n",
    "            'nrLongWords',],\n",
    "            by='nrSentences'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    ###Semantic metrics\n",
    "    semantic = [\n",
    "        divide(df, [\n",
    "            'nrDifficultWordsSAT',\n",
    "            'nrDifficultWordsDaleChall',\n",
    "            'nrSynsets'],\n",
    "            by='nrWords'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    ###Sentiment Enablers\n",
    "    sentiment = [\n",
    "        opinionMixScore(df),\n",
    "        nrAmbiguousSentimentWords(df),\n",
    "        nrStrongSentimentWords(df, strongWords),\n",
    "    ]\n",
    "\n",
    "    ###Readability formulas\n",
    "    formulas = [\n",
    "        formulaFleshKincaid(df),\n",
    "        formulaGunningFog(df),\n",
    "        formulaSMOG(df),\n",
    "        formulaDaleChall(df),\n",
    "        formulaColemanLiau(df),\n",
    "        formulaLinsearWrite(df),\n",
    "        formulaSpache(df),\n",
    "        formulaLIX(df),\n",
    "        formulaFORCAST(df),\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=['words'])\n",
    "\n",
    "    df.to_csv(outputFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = {\n",
    "    'base': list(itertools.chain(*base)),\n",
    "    'postag': list(itertools.chain(*postag)),\n",
    "    'postagwords': list(itertools.chain(*postagwords)),\n",
    "    'lexical': list(itertools.chain(*lexical)),\n",
    "    'syntactic': list(itertools.chain(*syntactic)),\n",
    "    'semantic': list(itertools.chain(*semantic)),\n",
    "    'sentiment': list(itertools.chain(*sentiment)),\n",
    "    'formulas': list(itertools.chain(*formulas)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dimensions, open('./Data-Dimensions.pickle', mode='wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}